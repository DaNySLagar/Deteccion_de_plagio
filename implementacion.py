# -*- coding: utf-8 -*-
"""implementacion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15voLdoJs6K6ZJsrbCGqaUuBK5TCYBWJj

# **Detección de plagio en documentos**

**Bibliotecas necesarias**
"""

!pip install PyPDF2

!pip install python-docx requests beautifulsoup4

"""**Librerias necesarias**"""

import nltk
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import re
from nltk.tokenize import TweetTokenizer

#leer PDF
import PyPDF2

#coseno de similitud
import numpy as np

#word
import docx

#WEB
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from googleapiclient.discovery import build
from difflib import SequenceMatcher

#detalle
import os

#divición
from nltk.tokenize import sent_tokenize

"""**Descargando el conjunto de stopwords**"""

nltk.download('stopwords')
nltk.download('punkt')

"""**Preprocesamiento de texto**"""

def process_text(text):

  #Tokenizar la cadena

  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
  text_tokens = tokenizer.tokenize(text)

  #Eliminar stop words y signos de puntuación

  stopwords_english = stopwords.words('english')
  text_clean = []

  for word in text_tokens:
      if (word not in stopwords_english):  # remove signos de puntuación
          text_clean.append(word)

  return ' '.join(text_clean) #array to string

"""**Extracción de información**"""

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

"""**Cargar los documentos**"""

pdf_files = ["/content/Documento 4.pdf", "/content/Documento 18.pdf", "/content/Documento 2.pdf"]

"""**Extraer y Procesar la información de los documentos**"""

preprocessed_texts = []
for doc in pdf_files:
    if doc.endswith('.pdf'):
        text = extract_text_from_pdf(doc)
    elif doc.endswith('.docx'):
        text = extract_text_from_docx(doc)

    preprocessed_text = process_text(text)
    preprocessed_texts.append(preprocessed_text)

"""**Matriz de los textos**"""

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

"""**Coseno de Similitud**"""

def coseno_similitud(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)

    if norm_vector1 != 0 and norm_vector2 != 0:
        similarity = dot_product / (norm_vector1 * norm_vector2)
    else:
        similarity = 0.0  # Si alguno de los vectores tiene longitud cero, la similitud es cero.

    return similarity

"""**Umbral dinamico**"""

def dynamic_threshold(length):
    # Ajusta el umbral según la longitud (en este ejemplo, usamos 1 / sqrt(longitud))
    return 1 / (length ** 0.5)

"""**Calculo de la longitud de textos**"""

text_lengths = [len(text.split()) for text in preprocessed_texts]

text_lengths

"""**Cálculo de similitud**"""

for i in range(len(pdf_files)):
    for j in range(i + 1, len(pdf_files)):
        similarity_score = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]

        # Obtener solo el nombre del archivo o URL sin la ruta completa
        file_name_i = os.path.basename(pdf_files[i])
        file_name_j = os.path.basename(pdf_files[j])

        # Calcular el umbral dinámico para cada texto
        threshold_i = dynamic_threshold(text_lengths[i])
        threshold_j = dynamic_threshold(text_lengths[j])

        static_threshold = 0.6
        threshold_i = max(static_threshold, threshold_i)
        threshold_j = max(static_threshold, threshold_j)

        similarity_percentage = similarity_score * 100

        # Mostrar el resultado de la comparación
        if similarity_score > 0 or similarity_score > 0:
            print(f"El documento {file_name_i} y el documento {file_name_j} posiblemente tienen plagio, Con un porcentaje de {similarity_percentage:.2f}%")
        else:
            print(f"El documento {file_name_i} y el documento {file_name_j} no tienen similitud. No hay plagio.")

"""**Implementación WEB**"""

API_KEY = "AIzaSyA4gkoXlKN4wfEAWyTdFx7shN5pdCT9HqE"
SEARCH_ENGINE_ID = "b3efb1284abf04927"

def search_google(query):
    service = build("customsearch", "v1", developerKey=API_KEY)
    res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID).execute()
    return res.get("items", [])

def get_page_content(url):
    response = requests.get(url)
    return response.text

# Función para calcular la similitud de coseno entre dos listas de oraciones
def cosine_similarity_score(sentences1, sentences2):
    vectorizer = TfidfVectorizer()

    tfidf_matrix1 = vectorizer.fit_transform(sentences1)
    tfidf_matrix2 = vectorizer.transform(sentences2)

    similarity_score = cosine_similarity(tfidf_matrix1, tfidf_matrix2)

    return similarity_score

def split_text_into_sentences(document_content):
    return sent_tokenize(document_content)

# Función para detectar plagio
def detect_plagiarism(document_content, num_results=100):

    document_sentences = split_text_into_sentences(document_content)

    for sentence in document_sentences:

        query = " ".join(sentence.split()[:10])  # Utiliza las primeras 10 palabras del documento como consulta

        search_results = search_google(query)[:num_results]

        for result in search_results:
            url = result["link"]
            page_content = get_page_content(url)
            soup = BeautifulSoup(page_content, "html.parser")
            page_text = soup.get_text()
            page_text = process_text(page_text)

            # Preprocesamiento del contenido de la página web para obtener oraciones individuales
            page_sentences = [sentence.strip() for sentence in page_text.split('.')]

            # Calcular la similitud de coseno entre las oraciones
            similarity_scores = cosine_similarity_score([sentence], page_sentences)

            # obtener el maximo
            max_similarity_score = np.max(similarity_scores)

            if max_similarity_score > 0.2:  # Ajusta este valor según tus necesidades
                print(f"Plagio detectado en {url} (Similitud de: {max_similarity_score:.2f})")


# Documento de ejemplo (reemplázalo con tu propio contenido)
document_content = "Se denomina ciencia a todo el conocimiento o saber constituido mediante la observación y el estudio sistemático y razonado de la naturaleza, la sociedad y el pensamiento."

detect_plagiarism(document_content)

from google.colab import files
import ipywidgets as widgets
from IPython.display import display, HTML

upload = None

def create_upload_widget():
    return widgets.FileUpload(accept=".pdf,.docx", multiple=True, description="Cargar archivos")

upload = create_upload_widget()

text_input = widgets.Textarea(placeholder="Ingresa tu texto aquí", layout=widgets.Layout(height="200px"))
options = ["Detectar Plagio", "Detectar Similitud"]
option_select = widgets.Dropdown(options=options, description="Selecciona una opción:")
button = widgets.Button(description="Detectar", layout=widgets.Layout(width="200px"))

def on_text_input_change(change):
    upload.disabled = bool(change.new.strip())

def on_upload_change(change):
    text_input.disabled = bool(change.new)

def on_reset_button_clicked(b):
    global upload  # Declarar la variable upload como global
    with output:
        output.clear_output()
        upload.value.clear()
        text_input.value = ""
        upload.disabled = False
        text_input.disabled = False
        print("Formulario restablecido")
    upload = create_upload_widget()

def on_button_clicked(b):
    with output:
        output.clear_output()
        option = option_select.value

        if option == "Detectar Similitud":
            if len(upload.value) >= 2:
                for file_name, file_content in upload.value.items():

                  with open(os.path.join('/content/', file_name), 'wb') as f:
                      f.write(file_content['content'])

            else:
                print("\nDebes de subir documentos (2)")

        elif option == "Detectar Plagio":
            if text_input.value.strip() != "":

                detect_plagiarism(text_input.value)

            elif len(upload.value) >= 2:
                for file_name, file_content in upload.value.items():

                  with open(os.path.join('/content/', file_name), 'wb') as f:
                      f.write(file_content['content'])
            else:
                print("\nSube documentos o Ingresa un texto")


reset_button = widgets.Button(description="X", layout=widgets.Layout(width="150px"))
reset_button.on_click(on_reset_button_clicked)

button.on_click(on_button_clicked)
text_input.observe(on_text_input_change, names='value')
upload.observe(on_upload_change, names='value')
output = widgets.Output()
display(HTML("<style>div.widget-box { max-width: 600px; }</style>"))

display(widgets.VBox([upload,reset_button, text_input, option_select, button, widgets.HBox([output])]))