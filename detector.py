# -*- coding: utf-8 -*-
"""Proyecto Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11bOJVhpEW54dF8-Derunlvb0MHBy-GPd

# **Detección de plagio en documentos**

**Bibliotecas necesarias**
"""

!pip install PyPDF2

"""**Librerias necesarias**"""

import nltk
import string
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import re
from nltk.tokenize import TweetTokenizer

#leer PDF
import PyPDF2

#coseno de similitud
import numpy as np

"""**Descargando el conjunto de stopwords**"""

nltk.download('stopwords')
nltk.download('punkt')

"""**Preprocesamiento de texto**"""

def process_text(text):

  #Eliminar hipervínculos, marcas y estilos de Twitter: Considerar todas las palabras

  #Tokenizar la cadena

  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
  text_tokens = tokenizer.tokenize(text)

  #Eliminar stop words y signos de puntuación

  stopwords_english = stopwords.words('english')
  text_clean = []

  for word in text_tokens:
      if (word not in stopwords_english and  # remove stopwords
          word not in string.punctuation):  # remove signos de puntuación
          text_clean.append(word)


  return ' '.join(text_clean) #array to string

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

def preprocess_text(text):
    # Tokenización y eliminación de puntuación
    translator = str.maketrans('', '', string.punctuation)
    tokens = nltk.word_tokenize(text.lower().translate(translator))

    # Eliminación de stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in tokens if word not in stop_words]

    # Lematización
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

    return ' '.join(lemmatized_words)

"""**Extracción de información**"""

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

"""**Cargar y extraer información del PDF**"""

pdf_1 = "/content/Documento 2.pdf"
pdf_3 = "/content/Documento 18.pdf"

text1 = extract_text_from_pdf(pdf_1)
text2 = extract_text_from_pdf(pdf_3)

"""**Procesar texto del PDF**"""

text1_preprocessed = process_text(text1)
text2_preprocessed = process_text(text2)
text2_preprocessed_1 = preprocess_text(text1)

"""**Texto sin procesar**"""

text1

"""**Texto procesado sin lematización**"""

text1_preprocessed

"""**Texto procesado con lematización**"""

text2_preprocessed_1

"""**Matriz de los textos**"""

# Crear el vectorizador TF-IDF
tfidf_vectorizer = TfidfVectorizer()

# Crear la matriz TF-IDF de los textos
tfidf_matrix = tfidf_vectorizer.fit_transform([text1_preprocessed, text2_preprocessed])

print(tfidf_matrix)

vector1 = tfidf_matrix[0].toarray()[0]
vector2 = tfidf_matrix[1].toarray()[0]

"""**Coseno de Similitud**"""

def coseno_similitud(vector1, vector2):
    dot_product = np.dot(vector1, vector2)
    norm_vector1 = np.linalg.norm(vector1)
    norm_vector2 = np.linalg.norm(vector2)

    if norm_vector1 != 0 and norm_vector2 != 0:
        similarity = dot_product / (norm_vector1 * norm_vector2)
    else:
        similarity = 0.0  # Si alguno de los vectores tiene longitud cero, la similitud es cero.

    return similarity

"""**Distancia Euclidiana**"""

def euclidean_distance(vectors1, vectors2):
    return np.linalg.norm(vectors1 - vectors2)

"""**Cálculo de similitud**"""

# Cálculo de la similitud del coseno entre los textos
similarity_score = coseno_similitud(vector1, vector2)

# Definir un umbral para determinar si los documentos son considerados plagio
threshold = 0.6

similarity_percentage = similarity_score * 100

# Verificar si los documentos son similares o no
if similarity_score > threshold:
    print(f"Los documentos son similares. Con un porcentaje de {similarity_percentage:.2f}% plagio/similitud")
else:
    print("Los documentos no son similares. No hay plagio.")

"""# **Umbral dinamico**"""

pdf_files = ["/content/Documento 2.pdf", "/content/Documento 4.pdf"]

"""**Función para el cálculo del Umbral dinámico**"""

def dynamic_threshold(length):
    # Ajusta el umbral según la longitud (en este ejemplo, usamos 1 / sqrt(longitud))
    return 1 / (length ** 0.5)

preprocessed_texts = []
for pdf_file in pdf_files:
    text = extract_text_from_pdf(pdf_file)
    preprocessed_text = preprocess_text(text)
    preprocessed_texts.append(preprocessed_text)

"""**Obteniendo el tamaño o longitud del texto**"""

text_lengths = [len(text.split()) for text in preprocessed_texts]

text_lengths

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

# Cálculo de la similitud del coseno entre los textos
similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]

# Calcular el umbral dinámico para cada texto
threshold_i = dynamic_threshold(text_lengths[0])
threshold_j = dynamic_threshold(text_lengths[1])

# Usar el umbral dinámico si es mayor que el umbral estático original
static_threshold = 0.6
threshold_i = max(static_threshold, threshold_i)
threshold_j = max(static_threshold, threshold_j)

similarity_percentage = similarity_score * 100

# Verificar si los documentos son similares o no
if similarity_score > threshold_i or similarity_score > threshold_j:
    print(f"Los documentos son similares. Con un porcentaje de {similarity_percentage:.2f}% plagio/similitud")
else:
    print("Los documentos no son similares. No hay plagio.")

"""**Cantidad de Documentos**"""

# Rutas a los documentos PDF que deseas comparar
pdf_files = ["/content/Documento 4.pdf", "/content/Documento 18.pdf", "/content/Documento 2.pdf"]

preprocessed_texts = []
for pdf_file in pdf_files:
    text = extract_text_from_pdf(pdf_file)
    preprocessed_text = preprocess_text(text)
    preprocessed_texts.append(preprocessed_text)

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

import os

# Verificar similitud entre todos los pares de documentos
for i in range(len(pdf_files)):
    for j in range(i + 1, len(pdf_files)):
        similarity_score = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]

        # Definir un umbral para determinar si los documentos son considerados plagio
        threshold = 0.6

        # Obtener solo el nombre del archivo sin la ruta completa
        file_name_i = os.path.basename(pdf_files[i])
        file_name_j = os.path.basename(pdf_files[j])


        # Mostrar el resultado de la comparación
        if similarity_score > threshold:
            print(f"El documento {file_name_i} y el documento {file_name_j} posiblemente tienen similitud. Posible plagio.")
        else:
            print(f"El documento {file_name_i} y el documento {file_name_j} no tienen similitud. No hay plagio.")

"""**Variabilidad de formatos (Word)**"""

!pip install python-docx requests beautifulsoup4

import docx

def extract_text_from_docx(docx_path):
    doc = docx.Document(docx_path)
    full_text = []
    for para in doc.paragraphs:
        full_text.append(para.text)
    return '\n'.join(full_text)

documents = [
    "/content/Documento 1.docx",
    "/content/Documento 2.pdf",
    "/content/Documento 3.docx"
]

# Extraer texto de los documentos y realizar el preprocesamiento
preprocessed_texts = []
for doc in documents:
    if doc.endswith('.pdf'):
        text = extract_text_from_pdf(doc)
    elif doc.endswith('.docx'):
        text = extract_text_from_docx(doc)

    preprocessed_text = preprocess_text(text)
    preprocessed_texts.append(preprocessed_text)

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

for i in range(len(documents)):
    for j in range(i + 1, len(documents)):
        similarity_score = cosine_similarity(tfidf_matrix[i], tfidf_matrix[j])[0][0]

        # Definir un umbral para determinar si los documentos son considerados plagio
        threshold = 0.8

        # Obtener solo el nombre del archivo o URL sin la ruta completa
        file_name_i = os.path.basename(documents[i])
        file_name_j = os.path.basename(documents[j])

        similarity_percentage = similarity_score * 100

        # Mostrar el resultado de la comparación
        if similarity_score > threshold:
            print(f"El documento {file_name_i} y el documento {file_name_j} posiblemente tienen plagio, Con un porcentaje de {similarity_percentage:.2f}%")
        else:
            print(f"El documento {file_name_i} y el documento {file_name_j} no tienen similitud. No hay plagio.")

"""**Implementación WEB**"""

import requests
from bs4 import BeautifulSoup
from difflib import SequenceMatcher
from googleapiclient.discovery import build

# Configura tu clave de API de Google Custom Search
API_KEY = "AIzaSyA4gkoXlKN4wfEAWyTdFx7shN5pdCT9HqE"
SEARCH_ENGINE_ID = "b3efb1284abf04927"

# Función para obtener resultados de búsqueda de Google
def search_google(query):
    service = build("customsearch", "v1", developerKey=API_KEY)
    res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID).execute()
    return res.get("items", [])

# Función para obtener el contenido de una URL
def get_page_content(url):
    response = requests.get(url)
    return response.text

# Función para comparar el contenido del documento con el contenido de los resultados de búsqueda
def detect_plagiarism(document_content, num_results=20):
    query = " ".join(document_content.split()[:10])  # Utiliza las primeras 10 palabras del documento como consulta

    search_results = search_google(query)[:num_results]

    for result in search_results:
        url = result["link"]
        page_content = get_page_content(url)
        soup = BeautifulSoup(page_content, "html.parser")
        page_text = soup.get_text()

        similarity_score = SequenceMatcher(None, query, page_text).ratio()


        if similarity_score > 0:
            print(f"Plagio detectado en {url} (Similitud: {similarity_score:.2f})")


# Documento de ejemplo (reemplázalo con tu propio contenido)
document_content = "Se denomina ciencia a todo el conocimiento o saber constituido mediante la observación y el estudio sistemático y razonado de la naturaleza, la sociedad y el pensamiento."

detect_plagiarism(document_content)

"""**Fase II: Implementación WEB**"""

import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from googleapiclient.discovery import build

# Configura tu clave de API de Google Custom Search
API_KEY = "AIzaSyA4gkoXlKN4wfEAWyTdFx7shN5pdCT9HqE"
SEARCH_ENGINE_ID = "b3efb1284abf04927"

# Función para obtener resultados de búsqueda de Google
def search_google(query):
    service = build("customsearch", "v1", developerKey=API_KEY)
    res = service.cse().list(q=query, cx=SEARCH_ENGINE_ID).execute()
    return res.get("items", [])

# Función para obtener el contenido de una URL
def get_page_content(url):
    response = requests.get(url)
    return response.text

# Función para calcular la similitud de coseno entre dos listas de oraciones
def cosine_similarity_score(sentences1, sentences2):
    vectorizer = TfidfVectorizer()
    tfidf_matrix1 = vectorizer.fit_transform(sentences1)
    tfidf_matrix2 = vectorizer.transform(sentences2)
    similarity_scores = cosine_similarity(tfidf_matrix1, tfidf_matrix2)
    return similarity_scores

# Función para detectar plagio
def detect_plagiarism(document_content, num_results=20):
    query = " ".join(document_content.split()[:10])  # Utiliza las primeras 10 palabras del documento como consulta

    search_results = search_google(query)[:num_results]

    # Preprocesamiento del contenido del documento y las consultas para obtener oraciones individuales
    document_sentences = [sentence.strip() for sentence in document_content.split('.')]
    query_sentences = [" ".join(query.split()[:10])]  # Utiliza las primeras 10 palabras del documento como consulta

    for result in search_results:
        url = result["link"]
        page_content = get_page_content(url)
        soup = BeautifulSoup(page_content, "html.parser")
        page_text = soup.get_text()

        # Preprocesamiento del contenido de la página web para obtener oraciones individuales
        page_sentences = [sentence.strip() for sentence in page_text.split('.')]

        # Calcular la similitud de coseno entre las oraciones
        similarity_scores = cosine_similarity_score(query_sentences, page_sentences)

        # Calcular la media de las similitudes
        mean_similarity_score = similarity_scores.mean()

        if mean_similarity_score > 0.2:  # Ajusta este valor según tus necesidades
            print(f"Plagio detectado en {url} (Similitud de coseno media: {mean_similarity_score:.2f})")



# Documento de ejemplo (reemplázalo con tu propio contenido)
document_content = "Se denomina ciencia a todo el conocimiento o saber constituido mediante la observación y el estudio sistemático y razonado de la naturaleza, la sociedad y el pensamiento."

detect_plagiarism(document_content)